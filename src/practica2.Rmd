---
title: "Práctica 2: limpieza, análisis y representación de los datos"
author: "Cristina Liánez López y Manuel Padrón Martínez"
date: '`r format(Sys.Date(),"%e de %B, %Y")`'
output:
  pdf_document:
    toc: yes
  html_document:
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r load_libraries, include=FALSE}
library(stringr)
library(VIM)


```

# 1. Detalles de la actividad

En esta actividad se elabora un caso práctico, consistente en el tratamiento de un conjunto de datos (en inglés, dataset), orientado a aprender a identificar los datos relevantes para un proyecto analítico y usar las herramientas de integración, limpieza, validación y análisis de las mismas.

# 2. Resolución 

### 2.1. Descripción del dataset 

El conjunto de datos objeto de análisis se ha obtenido a partir de un dataset libre disponible en Kaggle. 
Este conjunto de datos incluye información sobre diferentes marcas de vehículos nuevos y usados a la venta en los EE.UU. Los datos se obtuvieron haciendo uso de la técnica de _Web Scraping._ Está constituido por 2499 vehículos (filas o registros), de los que se han analizado 13 características (columnas) de cada uno.

Las características analizadas en este dataset son:

* **x**: valor para identificar las filas. Comienza en 0.

* **price**:	precio de venta del vehículo en $.

* **brand**:	marca del vehículo.

* **model**:  modelo del vehículo.

* **year**:	año de la primera matriculación del vehículo.

* **Title_Status**:	Esta característica incluye dos posibles valores: _clean title_ que significa que el vehículo es apto para circular; o _salvage insurance_ en caso de que no sea apto para circular debido a que está dañado por un accidente, inundación, incendio, o cualquier otra circunstancia.

* **Mileage**: kilometraje del vehículo, expresado en millas.

* **Color**:	Color del vehículo.

* **Vin**:  Número de bastidor. Compuesto por 17 caracteres (números y letras)

* **Lot**:	es un número de identificación asignado a una cantidad determinada o un lote de coches de un solo fabricante. En este caso, se combina un número de lote con un número de serie para formar el número de identificación del vehículo.

* **State**:	estado o ciudad donde se encuentra el vehículo.

* **Country**: país donde se encuentra el vehículo.

* **Condition**:	tiempo que hace que se publicó el anuncio de venta del vehículo en la página web.

### 2.2. Integración y selección de los datos de interés a analizar.


Preguntas a responder con el estudio:

* ¿Qué influye más en el precio de un vehículo con menos de 20 años de antigüedad: su antigüedad o el kilometraje que tenga?

* ¿Los coches de segunda mano de color blanco son más caros que los de color negro?

* ¿Podría crearse un modelo o fórmula para calcular el precio de venta de los vehículos de segunda mano, de manera objetiva, en función de ciertas características de los vehículos? ¿Cuáles serían las características más relevantes a tener en cuenta en esa fórmula?


### 2.3. Limpieza de los datos 

En primer lugar, procedemos a realizar la lectura del fichero en formato CSV en el que se encuentran los datos. A continuación, examinaremos el tipo de datos con los que R ha interpretado cada variable.

```{r chunck1}
# Carga del archivo
setwd("../csv")
cars <- read.csv("USA_cars_datasets.csv",header=TRUE)

#muestra las primeras filas del dataset
head(cars)

#Examino el tipo de datos de cada variable
str(cars)

#Miramos un resumen de los datos
summary(cars)
```

Puede observarse que los tipos de datos asignados automáticamente por R a las variables se corresponden con el dominio de estas.

De las 13 características registradas de cada vehículo, se ha decido prescindir de **x**, **lot** y **condition**, ya que no son atributos propios de los vehículos, sino que hacen referencia a los anuncios en los que se publicitaban a los mismos.

```{r chunck2}
# Prescindimos de las variables X, lot y condition
cars <- cars[,-(1)]
cars <- cars[,-(9)]
cars <- cars[,-(11)]
str(cars)
head(cars)

```

#### 2.3.1. Normalización de variables

A continuación, mostraremos los valores de las variables cualitativas o categóricas mediante el uso de tablas de frecuencia. Ésto nos permitirá saber si hay valores fuera del rango o valores extraños en ellas. 

```{r chunck3}
#variables cualitativas
table(cars$brand)
table(cars$model)
table(cars$title_status)
table(cars$color)
table(cars$state)
table(cars$country)
```

Los colores se van a clasificar en los siguientes valores: _beige, black, blue, brown, orange, gold, red, silver, white, gray, green, purple, yellow, no-color_ 

```{r chunck4}
cars$color <- str_replace(cars$color, ".*beige.*", "beige")
cars$color <- str_replace(cars$color, ".*black.*", "black")
cars$color <- str_replace(cars$color, ".*blue.*", "blue")
cars$color <- str_replace(cars$color, ".*brown.*", "brown")
cars$color <- str_replace(cars$color, ".*orange.*", "orange")
cars$color <- str_replace(cars$color, ".*gold.*", "gold")
cars$color <- str_replace(cars$color, ".*red.*", "red")
cars$color <- str_replace(cars$color, ".*silver.*", "silver")
cars$color <- str_replace(cars$color, ".*white.*", "white")
cars$color <- str_replace(cars$color, ".*gray.*", "gray")
cars$color <- str_replace(cars$color, ".*green.*", "green")
cars$color <- str_replace(cars$color, ".*purple.*", "purple")
cars$color <- str_replace(cars$color, ".*yellow.*", "yellow")
cars$color <- str_replace(cars$color, "burgundy", "red")
cars$color <- str_replace(cars$color, "charcoal", "black")
cars$color <- str_replace(cars$color, "color:", "no_color")
cars$color <- str_replace(cars$color, "maroon", "brown")
cars$color <- str_replace(cars$color, "magnetic metallic", "black")
cars$color <- str_replace(cars$color, "royal crimson metallic tinted clearcoat", "purple")
cars$color <- str_replace(cars$color, "turquoise", "blue")
cars$color <- str_replace(cars$color, "tan", "beige")
cars$color <- str_replace(cars$color, "guard", "black")

table(cars$color)

```

#### 2.3.2. Valores perdidos 

¿Los datos contienen ceros o elementos vacíos? ¿Cómo gestionarías cada uno de estos casos?

```{r chunck5}
# valores perdidos(0 o vacíos)
#Con la siguiente instrucción vemos si hay registros que están incompletos
complete.cases(cars)

#Analizamos valores perdidos
sapply(cars, function(x) sum(is.na(x)))
sapply(cars, function(x) sum(x == 0))
```

En primer lugar, hemos comprobado si hay registros incompletos, es decir, en los que en alguno de sus atributos no se haya introducido valor. Con la instrucciones ejecutada, se comprueba que no hay ninguno, ya que no se ha obtenido ningún valor a **FALSE**.

En segundo lugar, se ha analizado en cada una de las variables cuantitativas si existen valores almacenados equivalentes a 0 o NA. En el caso del precio, se han detectado 43 registros cuyo precio es 0; y para el caso del kilometraje, se han encontrado 6 registros con este mismo valor.

En el caso de la variable _price_, son claramente valores perdidos ya que no tiene sentido que el precio de venta fijado sea de 0$ cuando la naturaleza de los anuncios es la venta de los vehículos. Será necesario imputar los valores de estas variables en estos registros.

En el caso de la variable _mileage_ cuyo valor es 0, se ha mirado el valor del atributo _year_, puesto que si éste valor se corresponde con coches del 2020, el valor registrado en el atributo _mileage_ puede ser correcto ya que se trataría de vehículos nuevos que no han recorrido ninguna milla aún. 


```{r chunck6}

years <- subset(cars$year, subset = cars$mileage == 0)
print(years)

```

Tras realizar la comprobación, vemos que no es así en ninguno de los casos, es decir, son vehículos con cierta antigüedad, por lo que el valor a 0 de _mileage_ se corresponde con un valor perdido, que deberíamos de imputar.

Para la imputación de los valores perdidos se empleará un método basado en la similitud o diferencia entre los registros: la imputación basada en k vecinos más próximos (en inglés, kNN-imputation). La elección de esta alternativa se realiza bajo la hipótesis de que nuestros registros guardan cierta relación. No obstante, es mejor trabajar con datos “aproximados” que con los propios elementos vacíos, ya que obtendremos análisis con menor margen de error.

```{r chunck7}
#primero hay que sustituir los valores 0 por NA 
cars$price <- ifelse(cars$price == 0, NA, cars$price)
cars$mileage <- ifelse(cars$mileage == 0, NA, cars$mileage)
sapply(cars, function(x) sum(is.na(x)))

#imputamos los valores NA usando el método kNN
cars$price <- kNN(cars)$price
cars$mileage <- kNN(cars)$mileage

#comprobamos 
sapply(cars, function(x) sum(is.na(x)))
sapply(cars, function(x) sum(x == 0))


```


#### 2.3.3. Valores extremos.

Los valores extremos o outliers son aquellos que parecen no ser congruentes si los comparamos con el resto de los datos. Para identificarlos, utilizaremos la representación mediante un diagrama de caja.

```{r chunck8}
boxplot(cars$price)

boxplot(cars$year)

boxplot(cars$mileage)

summary(cars$price)
summary(cars$year)
summary(cars$mileage)
```

En los tres diagramas anteriores se observan bastantes valores outliers. Ésto es debido a que el rango de valores de las tres variables es bastante amplio. Si revisamos los datos y los comparamos con los valores resumen de cada variable, llegamos a la conclusión de que son valores posibles. Por tanto, se mantendrán tal y como están recogidos.

### 2.4. Análisis de los datos 

#### 2.4.1. Selección de los grupos de datos que se quieren analizar/comparar (planificación de los análisis a aplicar).

A continuación, se seleccionan los grupos dentro de nuestro conjunto de datos que pueden resultar interesantes para analizar y/o comparar.

```{r chunck9}
#Agrupar por año de primera matriculación, diferenciando los que tienen una antigüedad de 20 años o más.

year <- subset(cars$year, subset = cars$year >= 2000)
price <- subset(cars$price, subset = cars$year >= 2000)
mileage <- subset(cars$mileage, subset = cars$year >= 2000)
cochesMenos20 <- data.frame(price, year, mileage)
#head(cochesMenos20)

year <- subset(cars$year, subset = cars$year < 2000)
price <- subset(cars$price, subset = cars$year < 2000)
mileage <- subset(cars$mileage, subset = cars$year < 2000)
cochesMas20 <- data.frame(price, year, mileage)
#head(cochesMas20)


#Agrupar coches por color blanco o negro.

precioCochesBlancos <- subset(cars$price, subset = cars$color == "white")
precioCochesNegros <- subset(cars$price, subset = cars$color == "black")

```

#### 2.4.2. Comprobación de la normalidad y homogeneidad de la varianza.

Vamos a comprobar la suposición de normalidad y la homogeneidad de la varianza de las variables. Para ello, vamos a usar el test de Kolmogorov-Smirnov y luego lo contrastaremos con el test de Shapiro-Wilk. Con esta prueba si obtenemos un p-value mayor a 0,05 asumiremos que los datos siguen una distribución normal.

```{r chunck10}

#Comprobamos la normalidad de la variable año
ks.test(as.array(unique(cars$year)), pnorm, mean(as.array(unique(cars$year))), sd(as.array(unique(cars$year))))
shapiro.test(as.array(cars$year))

#Comprobamos la normalidad de la variable precio
ks.test(as.array(unique(cars$price)), pnorm, mean(as.array(unique(cars$price))), sd(as.array(unique(cars$price))))
shapiro.test(as.array(cars$price))

#Comprobamos la normalidad de la variable kilometraje
ks.test(as.array(unique(cars$mileage)), pnorm, mean(as.array(unique(cars$mileage))), sd(as.array(unique(cars$mileage))))
shapiro.test(as.array(cars$mileage))

#Comprobamos la normalidad de la variable precioCochesBlancos
ks.test(as.array(unique(precioCochesBlancos)), pnorm, mean(as.array(unique(precioCochesBlancos))), sd(as.array(unique(precioCochesBlancos))))
shapiro.test(as.array(precioCochesBlancos))

#Comprobamos la normalidad de la variable precioCochesNegros
ks.test(as.array(unique(precioCochesNegros)), pnorm, mean(as.array(unique(precioCochesNegros))), sd(as.array(unique(precioCochesNegros))))
shapiro.test(as.array(precioCochesNegros))


```

Como vemos, a excepción de la variable año, las otras 4 variables claramente no cumplen con la normalidad. En el caso de la variable año, debido al teorema central del límite, podemos considerar que añadiendo más observacioens tenderá a comportarse como una distribución normal.

Tambien vamos a hacer un análisis de homocedasticidad entre las variables precioCochesBlancos y precioCoches negros que son las que vamos a comprobar más adelante. Como nos han fallado los test de normalidad vamos a usar el test de Fligner-Killeen.

```{r chunck11}
fligner.test(price ~ color, data = cars)
```

Una vez más  nos da un valor por debajo de 0,05 por lo que se rechaza la hipótesis nula de homocedasticidad. Por lo que la variable price presenta varianzas diferentes para los diferentes grupos de colores.


#### 2.4.3. Aplicación de pruebas estadísticas para comparar los grupos de datos.

**a). Estudiar visualmente y analíticamente las posibles correlaciones entre:**

* las variables precio y el año de la primera matriculación.

* las variables precio y el kilometraje.

En los dos casos anteriores, solo se van a tener en cuenta los vehículos con una antigüedad inferior a 20 años, es decir, cuya fecha de primera matriculación sea igual o posterior al 2000.

```{r chunck13}
#price y year

plot(cochesMenos20$price, cochesMenos20$year, xlab = "Precio $", ylab = "Año primera matriculación")
abline(lm(cochesMenos20$year~cochesMenos20$price),col="red",lwd=4)
cor(x=cochesMenos20$price, y=cochesMenos20$year)

#price y mileage
plot(cochesMenos20$price, cochesMenos20$mileage, xlab = "Precio $", ylab = "Kilometraje en millas")
abline(lm(cochesMenos20$mileage~cochesMenos20$price),col="red",lwd=4)
cor(x=cochesMenos20$price, y=cochesMenos20$mileage)

```

Del análisis anterior se extrae que existe una correlación positiva entre las variables precio y año de primera matriculación. Es decir, cuanto mayor es el año de primera matriculación, más nuevo es el coche, mayor es el precio.

En cuanto a la relación entre las variables precio y kilometraje, ésta es negativa ya que un mayor kilometraje del vehículo, influye bajando el precio de venta del mismo.

Mediante la función cor(), que utiliza el coeficiente de correlación de Pearson, se observa que en ambos casos, la relación entre los pares de variables puede considerarse de fortaleza media, siendo mayor para la relación entre el precio y el año de primera matriculación. 

**b). Contraste de hipótesis **

Si hay un color que manda actualmente en el mercado de ocasión, ese es el blanco y no otros pigmentos que, tradicionalmente, han levantado más pasiones, como el negro y el rojo. Su espectacular repunte se puede atribuir a que el blanco es una pintura más económica pero, actualmente, la demanda supera ligeramente a la oferta, de modo que, ¿influirá el color del coche en el precio de venta en vehículos de segunda mano?

La siguiente prueba consistirá en un contraste de hipótesis sobre dos muestras para determinar si el precio del coche es superior si éste es color blanco a si lo es negro. Para ello, tendremos dos muestras: la primera de ellas se corresponderá con los precios de los coches de color blanco y, la segunda, con aquellos
que presentan el color negro.

Por aplicación del Teorema del límite central, para muestras con tamaño superior a 30, se puede suponer que los datos son normales. Como en este caso, n > 30, el contraste de hipótesis siguiente es válido.

Se plantea el siguiente **contraste de hipótesis de dos muestras sobre la diferencia de medias**:

   $H_{0}:\mu_{1} = \mu_{2}$
 
   $H_{1}:\mu_{1} > \mu_{2}$

donde $\mu_{1}$ es la media de la población de la que se extrae la primera muestra donde el color de los coches es blanco y $\mu_{2}$ es la media de la población de la que extrae la segunda donde el color de los coches es el negro.

Se trata de un test unilateral. Consideramos el nivel de significación $\alpha = 0.05$

```{r chunck14}
#Contraste de hipótesis color blanco Vs negro

t.test(precioCochesBlancos, precioCochesNegros, alternative = "less")
```

Dado que obtenemos un p-valor mayor que el nivel de significación $pValor > \alpha$, entonces no rechazamos la hipótesis nula. Por tanto, podemos concluir que el precio de un coche de color blanco no tiene por qué ser mayor al precio de un coche de color negro. 

**c).  Modelo de regresión **

A continuación, se va a crear un modelo o fórmula para predecir el precio de venta de los vehículos de segunda mano en función de ciertas características de los mismos. Así, se calcularán varios modelos de regresión lineal utilizando regresores cuantitativos, en el primero de ellos, y se añadirán regresores cualitativos a los anteriores, en los posteriores modelos, con los que poder realizar las predicciones de los precios.

En primer lugar, se estimará por mínimos cuadrados ordinarios un modelo lineal que explique la variable precio del vehículo en función del año de primera matriculación y el kilometraje. 

En los siguientes modelos de regresión lineal múltiple, se utilizarán regresores cuantitativos, los mismos que se han utilizado en el primer modelo, y se añadirán regresores cualitativos.

De entre los modelos que obtengamos, escogeremos el mejor utilizando como criterio aquel que presente un mayor coeficiente de determinación $R^{2}$.

```{r chunck15}
#predecir precio en función del año de matriculación y kilometraje
modelo1 <- lm(formula = cars$price ~ cars$year + cars$mileage)

#predecir precio en función del año de matriculación, kilometraje y marca
modelo2 <- lm(formula = cars$price ~ cars$year + cars$mileage + cars$brand)

#predecir precio en función del año de matriculación, kilometraje, marca y color
modelo3 <- lm(formula = cars$price ~ cars$year + cars$mileage + cars$brand + cars$color)

#Tabla con los coeficientes de determinación
coeficientes <- matrix(c( 1, summary(modelo1)$r.squared,
                         2, summary(modelo2)$r.squared,
                         3, summary(modelo3)$r.squared),
                         ncol = 2, byrow = TRUE)
print(coeficientes)


```

Podemos decir, que el tercer modelo es el más conveniente dado que tiene un mayor coeficiente de determinación. Empleando este modelo y haciendo uso de la **función predict()**, podemos realizar predicciones de precios de vehículos a partir del año de matriculación, kilometraje, marca y color. 

### 2.5. Representación de los resultados a partir de tablas y gráficas.





Para la diagnosis del modelo de regresión lineal múltiple escogido se harán dos gráficos: uno con los valores ajustados frente a los residuos (que nos permitirá ver si la varianza es constante) y el gráfico cuantil-cuantil que compara los residuos del modelo con los valores de una variable que se distribuye normalmente(QQ plot). 

```{r chunck19}
#Varianza de los errores/residuos constante
plot(fitted.values(modelo3),rstandard(modelo3), xlab="Valores ajustados", ylab="Residuos estandarizados")  
abline(h=0) 

#Distribución normal de los residuos
qqnorm(modelo3$residuals)
qqline(modelo3$residuals)

```

El **gráfico de los residuos en función de los valores ajustados por el modelo** permite evaluar 3 cuestiones principalmente:

* Si has utilizado el tipo de relación adecuada, es decir, si el modelo debería ser no lineal en lugar de lineal. Si el tipo de modelo que utilizaste no es el adecuado encontrarás sesgos o tendencias en los residuos.

* Si la varianza es constante o por el contrario tienes problemas de dispersión irregular. Los residuos deben distribuirse al azar alrededor del valor cero.

* Si existen datos extremos (outliers) que puedan perturbar e invalidar tu modelo. 

El **gráfico cuantil-cuantil (Normal Q-Q)** permite comparar la distribución de los residuos con la distribución normal teórica. Por lo tanto, si los residuos tienen una distribución normal deberían seguir aproximadamente la línea recta diagonal en el gráfico Q-Q normal, en caso contrario los residuos se van a apartar de la diagonal.

De las gráficas anteriores se puede extraer la presencia de outliers que están perturbando el modelo y valores alejados del comportamiento de la mayoría de los puntos (sobre todo en valores pequeños y altos de x).


### 2.6. Conclusiones

Finalmente, a partir de todo el estudio realizado, anotamos las siguientes conclusiones dando respuesta
a las preguntas inicialmente planteadas.

* ¿Qué influye más en el precio de un vehículo con menos de 20 años de antigüedad: su antigüedad o el kilometraje que tenga? 

Pues parece ser, que la antigüedad del vehículo tiene una repercusión mayor en el precio que el kilometraje de éste.

* ¿Los coches de color blanco son más caros que los de color negro?

En este estudio, y con un 95% de nivel de confianza, el precio de los coches de color blanco es similar al precio de los coches de color negro. 

* ¿Podría crearse un modelo o fórmula para calcular el precio de venta de los vehículos de segunda mano, de manera objetiva, en función de ciertas características de los vehículos? ¿Cuáles serían las características más relevantes a tener en cuenta en esa fórmula?

Sí, podemos realizar predicciones de precios de vehículos a partir del año de matriculación, kilometraje, marca y color. 

### 2.7. Crear el archivo procesado. 


```{r chunck20}

write.csv2(cars, file = "USA_cars_datasets_processed.csv")

```

# 3. Recursos

1. Calvo M., Subirats L., Pérez D. (2019). Introducción a la limpieza y análisis de los datos. Editorial UOC.

2. Dalgaard, P. (2008). Introductory statistics with R. Springer Science & Business Media.

3. Megan Squire (2015). Clean Data. Packt Publishing Ltd.

4. Jiawei Han, Micheine Kamber, Jian Pei (2012). Data mining: concepts and techniques. Morgan Kaufmann.

5. Jason W. Osborne (2010). Data Cleaning Basics: Best Practices in Dealing with Extreme Scores. Newborn and Infant Nursing Reviews; 10 (1): pp. 1527-3369.

6. Wes McKinney (2012). Python for Data Analysis. O’Reilley Media, Inc.

7. Vegas, E. (2017). Preprocesamiento de datos. Material UOC.

8. Gibergans, J. (2017). Regresión lineal múltiple. Material UOC.

9. Rovira, C. (2008). Contraste de hipótesis. Material UOC.


# 4. Tabla de contribuciones al trabajo

                      CONTRIBUCIONES        |        FIRMA
                  --------------------------|----------------------
                    Investigación previa    |      CLL, MPM
                    Redacción de respuestas |      CLL, MPM
                    Desarrollo código       |      CLL, MPM


